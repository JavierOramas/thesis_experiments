{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JYhwmjoQw6Aw",
        "outputId": "07bcdde6-56e4-4544-cde6-03cfb152b26c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: numpy in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: scipy in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: nltk in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /home/joramas/.local/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (65.5.0)\n",
            "Requirement already satisfied: wheel in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.38.4)\n",
            "Requirement already satisfied: cmake in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
            "Requirement already satisfied: click in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip install llama-index\n",
        "# !pip install GPT4All\n",
        "# !pip install einops\n",
        "# !pip install pypdf\n",
        "# !pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from typing import List\n",
        "\n",
        "nlp = spacy.load('es_core_news_lg')\n",
        "\n",
        "def remove_stop_words(text: str) -> str:\n",
        "  doc = nlp(text)\n",
        "  text_parts = [token.text for token in doc if not token.is_stop]\n",
        "  return \"\".join(text_parts)\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "  doc = nlp(text)\n",
        "  sentences = [sent.text for sent in doc.sents]\n",
        "  return sentences\n",
        "\n",
        "def group_sentences_semantically(sentences: List[str], threshold: int) -> List[str]:\n",
        "  docs = [nlp(sentence) for sentence in sentences]\n",
        "  segments = []\n",
        "\n",
        "  start_idx = 0\n",
        "  end_idx = 1\n",
        "  segment = [sentences[start_idx]]\n",
        "  while end_idx < len(docs):\n",
        "    if docs[start_idx].similarity(docs[end_idx]) >= threshold:\n",
        "      segment.append(docs[end_idx])\n",
        "    else:\n",
        "      segments.append(\" \".join(segment))\n",
        "      start_idx = end_idx\n",
        "      segment = [sentences[start_idx]]\n",
        "    end_idx += 1\n",
        "\n",
        "  if segment:\n",
        "    segments.append(\" \".join(segment))\n",
        "\n",
        "  return segments\n",
        "\n",
        "def split_text(text: str) -> List[str]:\n",
        "  text_no_stop_words = remove_stop_words(text)\n",
        "  sentences = split_sentences(text_no_stop_words)\n",
        "  return group_sentences_semantically(sentences, 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The semantic similarity between the two documents is: [0.6647494]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the cross-encoder model\n",
        "\n",
        "\n",
        "# Define the two documents\n",
        "doc1 = \"The University of Havana is one of the most important academic institutions in Cuba\"\n",
        "doc2 = \"There are many universities In the city of Havana, many of them are the best universities in the country\" \n",
        "# doc2 = \"Havana University has a very influent AI research group\"\n",
        "# doc2 = \"Holguin is a nice city\"\n",
        "\n",
        "\n",
        "# Compute the semantic similarity\n",
        "scores = model.predict([[doc1, doc2]])\n",
        "\n",
        "# Print the similarity score\n",
        "print(f\"The semantic similarity between the two documents is: {scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'examples': [<sentence_transformers.readers.InputExample.InputExample at 0x7f9a71dc3a50>,\n",
              "  <sentence_transformers.readers.InputExample.InputExample at 0x7f9a74d25010>]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EGT93zvVtEKS",
        "outputId": "9e46582b-b60a-4d05-fbb4-068cbe92d999"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     14\u001b[0m Path(local_path)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(local_path):\n\u001b[1;32m     17\u001b[0m   url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttp://gpt4all.io/models/\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     18\u001b[0m   response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# model = 'llama-2-7b-chat.ggmlv3.q4_0.bin\n",
        "model = 'ggml-model-gpt4all-falcon-q4_0.bin'\n",
        "# model = 'gpt4all-lora-quantized.bin'\n",
        "\n",
        "local_path = (\n",
        "    f\"./models/{model}\"  # replace with your desired local file path\n",
        ")\n",
        "\n",
        "import requests\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "  url = f'http://gpt4all.io/models/{model}'\n",
        "  response = requests.get(url, stream=True)\n",
        "\n",
        "  with open(local_path, 'wb') as f:\n",
        "      for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "          if chunk:\n",
        "              f.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ioT7Lo2vxUm2",
        "outputId": "7485c0b9-8952-489f-8340-7d9d0af29412"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLa ejecución de celdas con '/home/joramas/miniconda/envs/thesis/bin/python' requiere el paquete ipykernel.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -p /home/joramas/miniconda/envs/thesis ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from langchain.schema import language_model\n",
        "from langchain.llms import gpt4all\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding, ServiceContext, OpenAIEmbedding\n",
        "from llama_index.llms import LangChainLLM, HuggingFaceLLM\n",
        "from langchain.llms import gpt4all\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
        "# ckpt = 'google/pegasus-cnn_dailymail'\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
        "# model = EncoderDecoderModel.from_pretrained(ckpt).to(device)\n",
        "\n",
        "\n",
        "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
        "# query_wrapper_prompt = SimpleInputPrompt(\n",
        "#     \"Below is an instruction that describes a task. \"\n",
        "#     \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "#     \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
        "# )\n",
        "\n",
        "# ckpt=\"Writer/camel-5b-hf\"\n",
        "model = gpt4all.GPT4All(model=f\"./models/{model}\")\n",
        "\n",
        "model = LangChainLLM(llm=model)\n",
        "# model =  \n",
        "# model = HuggingFaceLLM(model=model)\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n",
        "\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=model,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=150\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memorándum del Seminario sobre Violencia y Paz al Consejo Asesor de la Comisión Ejecutiva de Atención a Víctimas\n",
        "\n",
        "https://rddm.mx/busqueda/#/docs?id=q524jq22z&has_model=ArchivalDocument&thumbnail=%2Fdownloads%2F1831cm38k%3Ffile%3Dthumbnail&related=1831cm38k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "pdf_path = \"./documents/FCE_0001_00060.pdf\"\n",
        "text = \"\"\n",
        "\n",
        "\n",
        "def translate_es_en(spanish_text):\n",
        "\n",
        "    # Load the pre-trained model and tokenizer\n",
        "    model_name = \"Helsinki-NLP/opus-mt-es-en\"  # Spanish to English translation model\n",
        "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the Spanish text\n",
        "    inputs = tokenizer.encode(spanish_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate translation\n",
        "    translated_ids = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "def translate_en_es(english_text):\n",
        "    # Load the pre-trained model and tokenizer\n",
        "    model_name = \"Helsinki-NLP/opus-mt-en-es\"  # English to Spanish translation model\n",
        "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the English text\n",
        "    inputs = tokenizer.encode(english_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate translation\n",
        "    translated_ids = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "def detect_topics_all(input):\n",
        "    mname = \"cristian-popa/bart-tl-all\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mname)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(mname).to(device)\n",
        "\n",
        "    # input = \"site web google search website online internet social content user\"\n",
        "    enc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
        "    outputs = model.generate(\n",
        "        input_ids=enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        max_length=25,\n",
        "        min_length=1,\n",
        "        do_sample=False,\n",
        "        num_beams=25,\n",
        "        length_penalty=1.0,\n",
        "        repetition_penalty=1.5\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Topic:\", decoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def detect_topics_ng(input):\n",
        "    mname = \"cristian-popa/bart-tl-ng\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mname)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(mname).to(device)\n",
        "\n",
        "    # input = \"site web google search website online internet social content user\"\n",
        "    enc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
        "    outputs = model.generate(\n",
        "        input_ids=enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        max_length=25,\n",
        "        min_length=1,\n",
        "        do_sample=False,\n",
        "        num_beams=25,\n",
        "        length_penalty=1.0,\n",
        "        repetition_penalty=1.5\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Topic:\", decoded)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp_es = spacy.load('es_core_news_lg')\n",
        "\n",
        "def filter_tokens(doc):\n",
        "    \n",
        "    doc = nlp_es(doc)\n",
        "    # print(len(doc))\n",
        "    filtered_tokens = [token.text for token in doc if not token.is_oov]\n",
        "    # print(len(filtered_tokens))\n",
        "    return \" \".join(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "def get_text():\n",
        "    pdf_path = \"./documents/FCE_0001_00060.pdf\"\n",
        "    \n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        \n",
        "        topics = []\n",
        "        \n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "    # return filter_tokens(text)            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: the concept\n",
            "Las masacres de San Fernando, Tamaulipas y Allende, Coahuila. Lecciones para el Estado mexicano, la sociedad organizada y las víctimas.\n",
            "Topic_ng\n",
            "\n",
            "----------\n",
            "Topic: human rights\n",
            "Se han realizado esfuerzos para acceder a la información de la Procuraduría General de Justicia (PGR), la Comisión Nacional de Derechos Humanos (CNDH), las comisiones de derechos humanos Tamaulipas y Coahuila, y las fiscalías locales.\n",
            "Topic_ng\n",
            "\n",
            "----------\n",
            "Topic: relevant\n",
            "El análisis deja al Estado mexicano muy mal. En general, aunque su grado y forma de responsabilidad varía, las autoridades no se comportan correctamente, y también ha dado seguimiento a los pasos que la víctima debe seguir ante la comunidad internacional.\n",
            "Topic_ng\n",
            "\n",
            "----------\n",
            "Topic: human rights\n",
            "Las organizaciones de la sociedad civil han viajado de P a a g i n a 2 de una actitud cautelosa pero receptiva a otra colaboración. Muestra de esto son los contactos establecidos con la Fundación por la Justicia y el Estado Democrático de Derecho.\n",
            "Topic_ng\n",
            "\n",
            "----------\n",
            "Topic: the concept\n",
            "Revisión de periódicos y revistas -centrándose en los casos y su contexto- y revisión y procesamiento de las bases de datos generadas por el Estado mexicano. El panorama es sombrío, la trágica situación en el país es evidente. Naturalmente, tragedia ha atraído los intereses académicos de otras instituciones.\n",
            "Topic_ng\n",
            "\n",
            "----------\n",
            "Topic: the concept\n",
            "La búsqueda de la justicia nunca será un asunto sencillo, pero siempre vale la pena intentarlo. El objetivo es contribuir a la búsqueda por la verdad, que es un paso indispensable hacia la Justicia.\n",
            "Topic_ng\n",
            "\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer \n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"  # English to Spanish translation model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load tokenizer and model\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"your-transformer-model-name\")\n",
        "# model = AutoModel.from_pretrained(\"your-transformer-model-name\")\n",
        "\n",
        "# Define maximum sequence length and batch size\n",
        "max_seq_length = 150 # Adjust as needed\n",
        "max_sum_length = 70\n",
        "# Tokenize the text\n",
        "text = get_text().split()\n",
        "\n",
        "i = 0\n",
        "batched_text = []\n",
        "\n",
        "for start_idx in range(0, len(text), max_seq_length):\n",
        "    sublist = text[start_idx:start_idx + max_seq_length]\n",
        "    batched_text.append(sublist)\n",
        "    \n",
        "pending_text = \"\"\n",
        "topics = set()\n",
        "for i in batched_text:\n",
        "    text = \" \".join(i)\n",
        "    en_text = translate_es_en(text)\n",
        "    pending_text += en_text\n",
        "    \n",
        "    if len(pending_text.split()) < max_sum_length:\n",
        "        continue\n",
        "    \n",
        "    text_to_summarize = pending_text\n",
        "    summary = summarizer(text_to_summarize, max_length=max_sum_length, min_length=30, do_sample=False)\n",
        "    es_text = translate_en_es(summary[0][\"summary_text\"])\n",
        "    \n",
        "    pending_text = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.file.base import SimpleDirectoryReader\n",
        "from llama_index import GPTVectorStoreIndex\n",
        "\n",
        "# other way of loading\n",
        "# from llama_index import download_loader\n",
        "# SimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")\n",
        "\n",
        "loader = SimpleDirectoryReader('./data', recursive=True, exclude_hidden=True)\n",
        "documents = loader.load_data()\n",
        "index = GPTVectorStoreIndex.from_documents(documents)\n",
        "index.query('What are these files about?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_hub.file.base import SimpleDirectoryReader\n",
        "from llama_index import GPTVectorStoreIndex\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from llama_index.llms import LangChainLLM, HuggingFaceLLM\n",
        "from langchain.llms import gpt4all\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# other way of loading\n",
        "# from llama_index import download_loader\n",
        "# SimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")\n",
        "\n",
        "loader = SimpleDirectoryReader('./data', recursive=True, exclude_hidden=True)\n",
        "documents = loader.load_data()\n",
        "index = GPTVectorStoreIndex.from_documents(documents)\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Local Directory Index\",\n",
        "        func=lambda q: index.query(q),\n",
        "        description=f\"Useful when you want answer questions about the files in your local directory.\",\n",
        "    ),\n",
        "]\n",
        "model = gpt4all.GPT4All(model=f\"./models/{model}\")\n",
        "\n",
        "model = LangChainLLM(llm=model)\n",
        "# model =  \n",
        "# model = HuggingFaceLLM(model=model)\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n",
        "\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=model,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=150\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "agent_chain = initialize_agent(\n",
        "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
        ")\n",
        "\n",
        "output = agent_chain.run(input=\"What are these files about?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Command K is a tool that lets you write code with an artificial intelligence (AI) program.',\n",
              " \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tgt_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForCausalLM were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "argument 'ids': 'dict' object cannot be converted to 'Sequence'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     10\u001b[0m expected_shape \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, inputs\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mvocab_size]\n\u001b[0;32m---> 11\u001b[0m tokenizer\u001b[39m.\u001b[39;49mdecode(outputs)\n",
            "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3525\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3523\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3525\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3526\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3527\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3528\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3529\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3530\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:546\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    545\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 546\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    548\u001b[0m clean_up_tokenization_spaces \u001b[39m=\u001b[39m (\n\u001b[1;32m    549\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    552\u001b[0m )\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n",
            "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'dict' object cannot be converted to 'Sequence'"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, PegasusForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n",
        "model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)\n",
        "assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n",
        "tokenizer.decode(outputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, PegasusXForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-large\")\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2846213366.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ARTICLE_TO_SUMMARIZE = \"Command K lets you edit and write code with the AI. To edit, try selecting some code, click \"Edit,\" and describe how the code should be changed. To generate completely new code, just type Command K without selecting anything.\"\u001b[0m\n\u001b[0m                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = tokenizer(ARTICLE_TO_SUMMARIZE,max_length=10000, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate Summary\n",
        "print(\"GENERATING SUMMARY\")\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], max_length=1000)\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: name 'pdf_path' is not defined\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        \n",
        "        topics = []\n",
        "        \n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            \n",
        "            \n",
        "            for p in page_text.split(\"\\n \"):\n",
        "                # if len(temp_text) > 512:\n",
        "                p_es = p\n",
        "                p = translate_es_en(p)\n",
        "                print(len(p.split()))\n",
        "                if len(p.split()) > 50:\n",
        "                    topic = detect_topics(p)\n",
        "                    text_to_summarize = f\"Topic: {topic} {p}\"\n",
        "                    output = summarizer(p, max_length=45, min_length=10, do_sample=False)[0][\"summary_text\"] + \"\\n\"\n",
        "                    topics.append(topic)\n",
        "                    print(\"Topic\", topic)\n",
        "                    output = translate_en_es(output)\n",
        "                    text += output\n",
        "                else:\n",
        "                    print(p)\n",
        "                    print(p_es)\n",
        "            # if 0 < len(temp_text) > 130:\n",
        "            #    text += summarizer(temp_text, max_length=130, min_length=30, do_sample=False) + \"\\n\"\n",
        "            \n",
        "            # print(\"Page\", page_num + 1, \":\", page_text)\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

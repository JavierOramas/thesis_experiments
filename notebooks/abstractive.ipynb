{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    %pip install spacy\n",
    "    import spacy\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "except:\n",
    "    %pip install sentence-transformers\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    \n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except:\n",
    "    %pip install transformers\n",
    "    from transformers import pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    %pip install torch\n",
    "    import torch\n",
    "    \n",
    "try: \n",
    "    import PyPDF2\n",
    "except:\n",
    "    %pip install pypdf\n",
    "    import PyPDF2\n",
    "\n",
    "try:\n",
    "    from llama_index import SimpleDirectoryReader\n",
    "except:     \n",
    "    %pip install llama-index\n",
    "    from llama_index import SimpleDirectoryReader\n",
    "    \n",
    "#     !pip install GPT4All\n",
    "#     !pip install einops\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Segmentation Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_es = spacy.load('es_core_news_lg')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def filter_tokens(doc):\n",
    "  # doc = nlp(doc)\n",
    "  # filtered_tokens = [token.text for token in doc if not token.is_oov]\n",
    "  # return \" \".join(filtered_tokens)\n",
    "  return doc\n",
    "  \n",
    "def remove_stop_words(text: str) -> str:\n",
    "  # doc = nlp(text)\n",
    "  # text_parts = [token.text for token in doc if not token.is_stop]\n",
    "  # return filter_tokens(\" \".join(text_parts))\n",
    "  \n",
    "  return text\n",
    "\n",
    "def split_paragraphs(text: str) -> List[str]:\n",
    "  paragraphs = []\n",
    "  doc = nlp(text)\n",
    "  current_paragraph = []\n",
    "  for token in doc:\n",
    "    # Check if the token is a newline character (end of paragraph)\n",
    "    if \"\\n\" in token.text:\n",
    "        # Append the current paragraph to the list of paragraphs\n",
    "        # if len(token.text) > 1:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "        current_paragraph = []\n",
    "        # Reset the current paragraph\n",
    "    else:\n",
    "        # Append the token's text to the current paragraph\n",
    "        current_paragraph.append(token.text)\n",
    "        \n",
    "  paragraphs.append(current_paragraph)\n",
    "  return paragraphs\n",
    "\n",
    "\n",
    "def split_sentences(text: list[str]) -> List[str]:\n",
    "  text = \" \".join(text)\n",
    "  doc = nlp(text)\n",
    "  sentences = [sent.text for sent in doc.sents]\n",
    "  return sentences\n",
    "\n",
    "def group_sentences_semantically(sentences: List[str], threshold: int) -> List[str]:\n",
    "  docs = [nlp(sentence) for sentence in sentences]\n",
    "  \n",
    "  segments = []\n",
    "\n",
    "  start_idx = 0\n",
    "  end_idx = 1\n",
    "  segment = [sentences[start_idx]]\n",
    "  model = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "  \n",
    "  while end_idx < len(docs):\n",
    "    similarity = model.predict([[str(docs[start_idx]), str(docs[end_idx])]])\n",
    "    if similarity >= threshold:\n",
    "      segment.append(str(docs[end_idx]))\n",
    "    else:\n",
    "      segments.append(\" \".join(segment))\n",
    "      start_idx = end_idx\n",
    "      segment = [sentences[start_idx]]\n",
    "    end_idx += 1\n",
    "\n",
    "  if segment:\n",
    "    segments.append(\" \".join(segment))\n",
    "  \n",
    "  del model\n",
    "  \n",
    "  return segments\n",
    "\n",
    "def split_text(text: str) -> List[str]:\n",
    "  text_no_stop_words = remove_stop_words(text)\n",
    "  # sentences = split_paragraphs(text_no_stop_words)\n",
    "  paragraphs = split_paragraphs(text_no_stop_words)\n",
    "  sentences = []\n",
    "  \n",
    "  for p in paragraphs:\n",
    "    print(\"paragraph\", p)\n",
    "    if len(p) > 100:\n",
    "      sentences.extend(group_sentences_semantically(split_sentences(p), 0.15))\n",
    "    else:\n",
    "      sentences.append(p)\n",
    "  # return group_sentences_semantically(sentences, 0.15)\n",
    "  return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_es_en(spanish_text):\n",
    "    device = 'cuda'\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    model_name = \"Helsinki-NLP/opus-mt-es-en\"  # Spanish to English translation model\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    # Tokenize the Spanish text\n",
    "    spanish_text_split = [spanish_text[i:i + 300] for i in range(0, len(spanish_text), 300)]\n",
    "    \n",
    "    translated_text = []\n",
    "    for text in spanish_text_split:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        translated_ids = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "        translated_text.append(tokenizer.decode(translated_ids[0], skip_special_tokens=True))\n",
    "    \n",
    "    return \" \".join(translated_text)\n",
    "\n",
    "def translate_en_es(english_text):\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-es\"  # English to Spanish translation model\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the English text\n",
    "    inputs = tokenizer.encode(english_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate translation\n",
    "    translated_ids = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def detect_topics_all(input):\n",
    "    mname = \"cristian-popa/bart-tl-all\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(mname).to(device)\n",
    "\n",
    "    # input = \"site web google search website online internet social content user\"\n",
    "    enc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=enc.input_ids,\n",
    "        attention_mask=enc.attention_mask,\n",
    "        max_length=25,\n",
    "        min_length=1,\n",
    "        do_sample=False,\n",
    "        num_beams=25,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Topic:\", decoded)\n",
    "    return decoded\n",
    "\n",
    "def detect_topics_ng(input):\n",
    "    mname = \"cristian-popa/bart-tl-ng\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(mname).to(device)\n",
    "\n",
    "    # input = \"site web google search website online internet social content user\"\n",
    "    enc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=enc.input_ids,\n",
    "        attention_mask=enc.attention_mask,\n",
    "        max_length=25,\n",
    "        min_length=1,\n",
    "        do_sample=False,\n",
    "        num_beams=25,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Topic:\", decoded)\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text():\n",
    "    documents = SimpleDirectoryReader(input_dir='documents', recursive=True).load_data()\n",
    "    \n",
    "    for i in documents:\n",
    "        doc = nlp_es(i.text)\n",
    "        text = \" \".join([token.text for token in doc])\n",
    "        # print(text)\n",
    "        yield translate_es_en(text), i.hash\n",
    "        \n",
    "    del documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "import os\n",
    "import json\n",
    "\n",
    "it = 0\n",
    "for t, id in get_text():    \n",
    "    if not os.path.exists(f\"processed_documents/{it}_{id}.json\"):\n",
    "        \n",
    "        text = split_text(t)\n",
    "        with open(f\"processed_documents/{it}_{id}.json\", \"w\") as f:\n",
    "            json.dump(text, f)\n",
    "            it += 1\n",
    "    text = []\n",
    "\n",
    "del nlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FALCON LLM summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpt4all import GPT4All\n",
    "\n",
    "#     # model_name='ggml-model-gpt4all-falcon-q4_0.bin',\n",
    "# model = GPT4All(\n",
    "#     model_name='llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "#     model_path='./models'\n",
    "#     )\n",
    "# for t in text:\n",
    "#     prompt = f\"summarize in one sentence: {t}\"\n",
    "#     print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "# summary_text = []\n",
    "\n",
    "# src_files = []\n",
    "# for file in os.listdir(\"processed_documents\"):\n",
    "#     if file.endswith(\".json\"):\n",
    "#         with open(os.path.join(\"processed_documents\", file), \"r\") as f:\n",
    "#             src_text = json.load(f)\n",
    "\n",
    "#             for sentence in src_text:\n",
    "                \n",
    "#                 inputs = tokenizer.encode(\"summarize: \" + sentence, return_tensors=\"pt\", max_length=512).to(device)\n",
    "#                 outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#                 summary = tokenizer.decode(outputs[0])\n",
    "#                 summary_text.append(summary)\n",
    "\n",
    "# print(\"\\n\\nSummarized text :\\n\",summary_text)\n",
    "# del model\n",
    "# del tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus xsum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src_text = split_text(get_text())\n",
    "\n",
    "del nlp\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"summarizing\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"generating\")\n",
    "\n",
    "batch = tokenizer(src_text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "translated = model.generate(**batch)\n",
    "\n",
    "print(\"decoding\")\n",
    "\n",
    "del model\n",
    "\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_name = \"google/pegasus-xsum\"\n",
    "# model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "# from summarizer import Summarizer\n",
    "\n",
    "# # model_name = 'bert-large-uncased'\n",
    "# # tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# # summarizer = Summarizer(model=model_name)\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# # Load tokenizer and model\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"your-transformer-model-name\")\n",
    "# # model = AutoModel.from_pretrained(\"your-transformer-model-name\")\n",
    "# device = 'cuda'\n",
    "# # Define maximum sequence length and batch size\n",
    "# # max_seq_length = 170 0 # Adjust as needed\n",
    "# # max_sum_length = 10\n",
    "# # Tokenize the text¨\n",
    "\n",
    "# summary_text = []\n",
    "# pending_text = \"\"\n",
    "\n",
    "# src_files = []\n",
    "# for file in os.listdir(\"processed_documents\"):\n",
    "#     if file.endswith(\".json\"):\n",
    "#         with open(os.path.join(\"processed_documents\", file), \"r\") as f:\n",
    "#             print(file)\n",
    "#             src_text = json.load(f)\n",
    "\n",
    "#             for sentence in src_text:\n",
    "                \n",
    "#                 # print(\"TEXT\", i)\n",
    "#                 if isinstance(sentence, list):\n",
    "#                     es_text = \" \".join(sentence)\n",
    "#                 else:\n",
    "#                     es_text = str(sentence)\n",
    "                    \n",
    "#                 en_text = translate_es_en(es_text)\n",
    "#                 # print(\"EN TEXT\", en_text)\n",
    "#                 pending_text += en_text\n",
    "                \n",
    "#                 text_to_summarize = pending_text\n",
    "                \n",
    "#                 max_summary_len = len(text_to_summarize.split(\" \"))\n",
    "#                 if max_summary_len < 10:\n",
    "#                     print(\"Short Sentence: \", es_text)\n",
    "                \n",
    "#                 else:\n",
    "#                     min_summary_len = min(max(int(max_summary_len/2), 1), 30)\n",
    "                    \n",
    "#                     print(min_summary_len, max_summary_len)    \n",
    "#                     summary = summarizer(text_to_summarize, max_length=max_summary_len, min_length=min_summary_len)[0][\"summary_text\"]\n",
    "\n",
    "#                     es_text = translate_en_es(summary)\n",
    "#                     print(\"SUMMARY: \", es_text)\n",
    "                \n",
    "#                 pending_text = \"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/normal-computing/outlines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 2 Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2-large/resolve/main/tf_model.h5 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb77960cad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    961\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 962\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7fb77960cad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2-large/resolve/main/tf_model.h5 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb77960cad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2-large\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Load pre-trained model (weights)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgpt2-large\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Set the model in evaluation mode to deactivate the DropOut modules\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# This is IMPORTANT to have reproducible results during evaluation!\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:2559\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m     \u001b[39m# Otherwise, maybe there is a TF or Flax model file.  We try those to give a helpful error\u001b[39;00m\n\u001b[1;32m   2553\u001b[0m     \u001b[39m# message.\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m     has_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   2555\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrevision\u001b[39m\u001b[39m\"\u001b[39m: revision,\n\u001b[1;32m   2556\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mproxies\u001b[39m\u001b[39m\"\u001b[39m: proxies,\n\u001b[1;32m   2557\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse_auth_token\u001b[39m\u001b[39m\"\u001b[39m: token,\n\u001b[1;32m   2558\u001b[0m     }\n\u001b[0;32m-> 2559\u001b[0m     \u001b[39mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhas_file_kwargs):\n\u001b[1;32m   2560\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2561\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2562\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[39m \u001b[39mvariant)\u001b[39m}\u001b[39;00m\u001b[39m but there is a file for TensorFlow weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2563\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2564\u001b[0m         )\n\u001b[1;32m   2565\u001b[0m     \u001b[39melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhas_file_kwargs):\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/transformers/utils/hub.py:610\u001b[0m, in \u001b[0;36mhas_file\u001b[0;34m(path_or_repo, filename, revision, proxies, use_auth_token)\u001b[0m\n\u001b[1;32m    607\u001b[0m url \u001b[39m=\u001b[39m hf_hub_url(path_or_repo, filename\u001b[39m=\u001b[39mfilename, revision\u001b[39m=\u001b[39mrevision)\n\u001b[1;32m    608\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(use_auth_token\u001b[39m=\u001b[39muse_auth_token, user_agent\u001b[39m=\u001b[39mhttp_user_agent())\n\u001b[0;32m--> 610\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mhead(url, headers\u001b[39m=\u001b[39;49mheaders, allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m    611\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/api.py:100\u001b[0m, in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a HEAD request.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mhead\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2-large/resolve/main/tf_model.h5 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb77960cad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "model.to(device)\n",
    "\n",
    "# Summarize text\n",
    "def summarize_text_gpt2(text):\n",
    "    # Encode a text inputs\n",
    "    indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Convert indexed tokens in a PyTorch tensor\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # Get the predicted next sub-word\n",
    "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Document 1...\", \"Document 2...\", ...]\n",
    "input_texts = [\"Summarize: \" + doc for doc in documents]\n",
    "inputs = tokenizer.batch_encode_plus(input_texts, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_summary_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "for i, summary_id in enumerate(summary_ids):\n",
    "    summary = tokenizer.decode(summary_id, skip_special_tokens=True)\n",
    "    print(f\"Summary {i+1}: {summary}\")\n",
    "# Remember that GPT-2 may not always produce perfectly coherent or accurate summaries, especially for complex documents. It's a good idea to experiment with different parameters and possibly fine-tuning to achieve the best results for your specific use case.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

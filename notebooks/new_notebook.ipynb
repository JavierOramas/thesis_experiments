{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install keypartx[coreferee_spacy]\n",
    "# !python3 -m coreferee install en \n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !pip install keypartx\n",
    "# # !python -m spacy download en_core_web_lg\n",
    "# !pip install git+https://github.com/pengKiina/pyvis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.query_engine import CitationQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joramas/miniconda/envs/thesis/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/joramas/miniconda/envs/thesis/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/joramas/miniconda/envs/thesis/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/joramas/miniconda/envs/thesis/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap, hdbscan\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "# no need for clean data which has removed stopwords and number, stemmatized extra...\n",
    "# my_list_of_stopwords = text.ENGLISH_STOP_WORDS.union(stopwords_fi +stopwords_sv)\n",
    "#vectorizer_model_multi = CountVectorizer(stop_words=my_list_of_stopwords, ngram_range=(1, 2), min_df=2) # finnish + english \n",
    "#vectorizer_model = CountVectorizer(stop_words=my_list_of_stopwords, ngram_range=(1,2), min_df=2) #The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=2) #The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory\n",
    "\n",
    "\n",
    "# Load sentence transformer model\n",
    "# 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")# english default \"all-MiniLM-L6-v2\"   \n",
    "#embedding_model= pipeline('fill-mask', model='Finnish-NLP/roberta-large-finnish')\n",
    "#embedding_model_multi = pipeline('fill-mask', model='bert-base-multilingual-cased') # multilingual bert\n",
    "embedding_model_multi = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\") # multingual ST\n",
    "\n",
    "\n",
    "# Create documents embeddings\n",
    "#embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "# Define UMAP model to reduce embeddings dimension\n",
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                        n_components=1,\n",
    "                       min_dist=0.0,\n",
    "                       metric='cosine',\n",
    "                       low_memory=False \n",
    "                       ) #, low_memory=False # to reduce topic numbers, UMAP model and set n_neighbors much higher than the default 15 (e.g., 200). \n",
    "\n",
    "# Define HDBSCAN model to perform documents clustering\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=5,\n",
    "                                min_samples=1,\n",
    "                                metric='euclidean',\n",
    "                                cluster_selection_method='eom',\n",
    "                                prediction_data=True) # the bigger cluster size for bigger documents \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Coreference package avaiable\n"
     ]
    }
   ],
   "source": [
    "from keypartx.basemodes.avn_base import lemma_en\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# text_link = \"https://raw.githubusercontent.com/pengKiina/KeypartX/main/data/Thailand_text.csv\"\n",
    "# commentsDF = pd.read_csv(text_link)\n",
    "# import re\n",
    "# comments_en = commentsDF.review.to_list()\n",
    "# cleaned_text = []\n",
    "# for comment in comments_en:\n",
    "#     comment_nonStop =  \" \".join([word for word in comment.split() if len(word)>3])\n",
    "#     comment_lemma = lemma_en(comment_nonStop)\n",
    "#     comment_nonum = re.sub(r'[0-9]+',' ', comment_lemma)\n",
    "#     cleaned_text.append(comment_nonum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BERTopic model EN\n",
    "# topic_model_en = BERTopic(top_n_words=10,\n",
    "#                        calculate_probabilities=False,\n",
    "#                        umap_model= umap_model,\n",
    "#                        hdbscan_model=hdbscan_model,\n",
    "#                        vectorizer_model=vectorizer_model,\n",
    "#                        embedding_model= embedding_model_multi,\n",
    "#                        verbose=True)\n",
    "# #topic_model_en = BERTopic()\n",
    "# topic_model_en.max_seq_length = 510\n",
    "\n",
    "\n",
    "# review_train = cleaned_text\n",
    "\n",
    "# # Train model, extract topics and probabilities\n",
    "# topics_en, probabilities_en = topic_model_en.fit_transform(review_train)\n",
    "# #topics, _ = topic_model.fit_transform(reviews_en)\n",
    "# topic_terms = pd.DataFrame(topic_model_en.get_topics())\n",
    "# topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "# model = CrossEncoder('https://huggingface.co/cross-encoder/quora-roberta-large')\n",
    "# scores = model.predict([('Question 1', 'Question 2'), ('Question 3', 'Question 4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load the model\n",
    "# model = Model(model_path=\"gpt4all-lora-quantized.bin\", n_ctx=2000)\n",
    "\n",
    "#Generate\n",
    "# prompt=\"User: How are you doing?\\nBot:\"\n",
    "\n",
    "# result=model.generate(prompt,n_predict=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface\u001b[39;00m \u001b[39mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m LangchainEmbedding, ServiceContext\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm\u001b[39;00m \u001b[39mimport\u001b[39;00m LangchainLLM\n\u001b[1;32m      5\u001b[0m \u001b[39m# load language model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# language_model = gpt4all.GPT4All(model=\"./llama-2-7b-chat.ggmlv3.q4_0.bin\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# language_model = gpt4all.GPT4All(model=\"./llama-2-7b-chat.ggmlv3.q4_0.bin\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m language_model \u001b[39m=\u001b[39m LangchainLLM( llm\u001b[39m=\u001b[39mgpt4all\u001b[39m.\u001b[39mGPT4All(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./ggml-model-gpt4all-falcon-q4_0.bin\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.llm'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import gpt4all\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.llm import LangchainLLM\n",
    "# load language model\n",
    "# language_model = gpt4all.GPT4All(model=\"./llama-2-7b-chat.ggmlv3.q4_0.bin\")\n",
    "# language_model = gpt4all.GPT4All(model=\"./llama-2-7b-chat.ggmlv3.q4_0.bin\")\n",
    "language_model = LangchainLLM( llm=gpt4all.GPT4All(model=\"./ggml-model-gpt4all-falcon-q4_0.bin\"))\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=language_model,\n",
    "    embed_model = embed_model,\n",
    "    chunk_size=100\n",
    ")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 19:03:47,109] {loading.py:63} INFO - Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# https://qdrant.tech/documentation/integrations/llama-index/\n",
    "if not os.path.exists(\"./citation\"):\n",
    "    documents = SimpleDirectoryReader(\"documents/paul_graham\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    index.storage_context.persist(persist_dir=\"./citation\")\n",
    "else:\n",
    "    index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./citation\"),\n",
    "        service_context=service_context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine(streaming=True)\n",
    "\n",
    "# streaming_response = query_engine.query(\"Who is Paul Graham.\")\n",
    "# streaming_response.print_response_stream() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    # here we can control how granular citation sources are, the default is 512\n",
    "    citation_chunk_size=52,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0eabfbe7e764f1a86eeedd67cd11ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The prompt size exceeds the context window size and cannot be processed.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "# source nodes are 6, because the original chunks of 1024-sized nodes were broken into more granular nodes\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

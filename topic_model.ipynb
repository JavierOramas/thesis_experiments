{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9f60c2ec934b34a1a5178c5bb4a394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e93b2d14ab401f9c9bbe175c58ac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Model file './models/mistral-7b-instruct-v0.1.Q5_K_M.gguf' not found in '/home/joramas/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mctransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheBloke/Mistral-7B-Instruct-v0.1-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/ctransformers/hub.py:168\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_dir(\n\u001b[1;32m    165\u001b[0m         model_path_or_repo_id, model_file\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_model_path_from_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(\n\u001b[1;32m    176\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m    177\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m    178\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    179\u001b[0m     lib\u001b[38;5;241m=\u001b[39mlib,\n\u001b[1;32m    180\u001b[0m )\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/ctransformers/hub.py:209\u001b[0m, in \u001b[0;36mAutoModelForCausalLM._find_model_path_from_repo\u001b[0;34m(cls, repo_id, filename, local_files_only, revision)\u001b[0m\n\u001b[1;32m    202\u001b[0m allow_patterns \u001b[38;5;241m=\u001b[39m filename \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    203\u001b[0m path \u001b[38;5;241m=\u001b[39m snapshot_download(\n\u001b[1;32m    204\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    205\u001b[0m     allow_patterns\u001b[38;5;241m=\u001b[39mallow_patterns,\n\u001b[1;32m    206\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    207\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    208\u001b[0m )\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_model_path_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/ctransformers/hub.py:242\u001b[0m, in \u001b[0;36mAutoModelForCausalLM._find_model_path_from_dir\u001b[0;34m(cls, path, filename)\u001b[0m\n\u001b[1;32m    240\u001b[0m     file \u001b[38;5;241m=\u001b[39m (path \u001b[38;5;241m/\u001b[39m filename)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(file)\n\u001b[1;32m    245\u001b[0m files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    246\u001b[0m     (f\u001b[38;5;241m.\u001b[39mstat()\u001b[38;5;241m.\u001b[39mst_size, f)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m path\u001b[38;5;241m.\u001b[39miterdir()\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;129;01mand\u001b[39;00m (f\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    249\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Model file './models/mistral-7b-instruct-v0.1.Q5_K_M.gguf' not found in '/home/joramas/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1'"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", \n",
    "    model_file=\"./models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\", \n",
    "    model_type=\"mistral\", \n",
    "    gpu_layers=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence.split(\"|\")[-1]\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        sentences[i] = sentence.split(\"|\")[-1]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load the dataset from a folder\n",
    "dataset_folder = \"pipeline/topic_model/test_dataset\"\n",
    "sentences = []\n",
    "\n",
    "# Iterate over files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(dataset_folder, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            sentences.extend(preprocess_text(text))\n",
    "\n",
    "\n",
    "\n",
    "print(f'length of documents: {len(sentences)}')\n",
    "print(f\"first sentences:\\n\", \"\\n\".join(sentences[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    embeddings.append(embed_model.embed(sentence))\n",
    "\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "\n",
    "# embeddings = umap.UMAP(n_neighbors=15, \n",
    "#                     n_components=10, \n",
    "#                     min_dist=0.0, \n",
    "#                     metric='cosine',\n",
    "#                    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "scores = []\n",
    "for k in range(5,6):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)\n",
    "    score = silhouette_score(embeddings, kmeans.labels_)\n",
    "    scores.append(score)\n",
    "\n",
    "best_k = scores.index(max(scores)) + 5 \n",
    "documents = KMeans(n_clusters=best_k, random_state=42).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "# select the topic words and decode them\n",
    "topic_words = []\n",
    "for i in range(best_k):\n",
    "    topic_words.append([embed_model.decode(word) for word in embeddings[documents.labels_ == i].mean(axis=0).argsort()[-10:][::-1]])\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k * vocab\n",
    "X_per_cluster = model.transform(documents)\n",
    "# D * vocab\n",
    "X_origin = self.vectorizer_model.transform(origin_documents)\n",
    "\n",
    "if self.word_select_method == 'tfidf_idfi':\n",
    "    socres = TFIDF_IDFi(X_per_cluster, X_origin, documents).socre()\n",
    "elif self.word_select_method == 'tfidf_tfi':\n",
    "    socres = TFIDF_TFi(X_per_cluster, X_origin, all_documents).socre()\n",
    "elif self.word_select_method == 'tfi':\n",
    "    socres = TFi(X_per_cluster).socre()\n",
    "elif self.word_select_method == 'tfidfi':\n",
    "    socres = TFIDFi(X_per_cluster).socre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

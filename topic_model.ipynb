{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joramas/miniconda/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", \n",
    "    model_file=\"/run/media/joramas/DATA/THESIS/playground/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\", \n",
    "    model_type=\"mistral\", \n",
    "    gpu_layers=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of documents: 107599\n",
      "first sentences:\n",
      " breast cancer risk test devised httpbbc\n",
      "gp workload harming care  bma poll httpbbc\n",
      "short peoples heart risk greater httpbbc\n",
      "new approach against hiv promising httpbbc\n",
      "coalition undermined nhs  doctors httpbbc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence.split(\"|\")[-1]\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        sentences[i] = sentence.split(\"|\")[-1]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load the dataset from a folder\n",
    "dataset_folder = \"pipeline/topic_model/test_dataset\"\n",
    "sentences = []\n",
    "\n",
    "# Iterate over files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(dataset_folder, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            sentences.extend(preprocess_text(text))\n",
    "\n",
    "\n",
    "\n",
    "print(f'length of documents: {len(sentences)}')\n",
    "print(f\"first sentences:\\n\", \"\\n\".join(sentences[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 13886, 8875, 4623, 1369, 1847, 2458, 3550, 1754, 28717], [1, 319, 28720, 771, 1768, 6241, 288, 1656, 28705, 287, 705, 8175, 3550, 1754, 28717], [1, 2485, 22529, 3031, 4623, 6517, 3550, 1754, 28717], [1, 633, 4431, 1835, 295, 449, 22449, 3550, 1754, 28717], [1, 28131, 640, 858, 1311, 307, 6270, 28705, 13500, 3550, 1754, 28717]]\n"
     ]
    }
   ],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    embeddings.append(embed_model.embed(sentence))\n",
    "\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2485, 22529, 3031, 4623, 6517, 3550, 1754, 28717]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (107599,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39;49mk, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(embeddings)\n\u001b[1;32m     16\u001b[0m     score \u001b[39m=\u001b[39m silhouette_score(embeddings, kmeans\u001b[39m.\u001b[39mlabels_)\n\u001b[1;32m     17\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1471\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1445\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \n\u001b[1;32m   1447\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1472\u001b[0m         X,\n\u001b[1;32m   1473\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1474\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1475\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1476\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1477\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1478\u001b[0m     )\n\u001b[1;32m   1480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1482\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (107599,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "\n",
    "# embeddings = umap.UMAP(n_neighbors=15, \n",
    "#                     n_components=10, \n",
    "#                     min_dist=0.0, \n",
    "#                     metric='cosine',\n",
    "#                    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "scores = []\n",
    "for k in range(5,6):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)\n",
    "    score = silhouette_score(embeddings, kmeans.labels_)\n",
    "    scores.append(score)\n",
    "\n",
    "best_k = scores.index(max(scores)) + 5 \n",
    "documents = KMeans(n_clusters=best_k, random_state=42).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_clusters': 5,\n",
       " 'init': 'k-means++',\n",
       " 'max_iter': 300,\n",
       " 'tol': 0.0001,\n",
       " 'n_init': 'warn',\n",
       " 'verbose': 0,\n",
       " 'random_state': 42,\n",
       " 'copy_x': True,\n",
       " 'algorithm': 'lloyd',\n",
       " 'n_features_in_': 10,\n",
       " '_tol': 0.000643173360824585,\n",
       " '_n_init': 10,\n",
       " '_algorithm': 'lloyd',\n",
       " '_n_threads': 6,\n",
       " 'cluster_centers_': array([[ 8.648771 ,  2.9892354,  8.056784 ,  1.4812537,  3.6511312,\n",
       "          3.829946 ,  1.9802415,  8.671658 ,  5.114523 ,  4.1643057],\n",
       "        [ 9.655565 ,  4.249937 ,  4.2360334,  1.9627676, 18.252346 ,\n",
       "          5.0754657,  4.3221054,  4.6452446,  5.5084286,  5.4275656],\n",
       "        [ 7.258113 , 19.762693 ,  6.308504 ,  3.41842  ,  4.9693966,\n",
       "          5.3060803,  5.550563 ,  5.112668 ,  7.8142514,  4.569195 ],\n",
       "        [ 9.942116 ,  4.468465 ,  3.7789404,  3.2886782,  4.557868 ,\n",
       "          5.314192 ,  5.188673 ,  4.111411 ,  4.2608705,  4.9704103],\n",
       "        [ 9.739618 ,  3.7230263,  4.303022 ,  3.1084948,  2.9522576,\n",
       "          3.1164143, 17.178328 ,  9.0201435, 11.967562 ,  4.3536453]],\n",
       "       dtype=float32),\n",
       " '_n_features_out': 5,\n",
       " 'labels_': array([3, 3, 3, ..., 3, 1, 3], dtype=int32),\n",
       " 'inertia_': 4125422.5,\n",
       " 'n_iter_': 18}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m topic_words \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(best_k):\n\u001b[0;32m----> 8\u001b[0m     topic_words\u001b[39m.\u001b[39mappend([embed_model\u001b[39m.\u001b[39;49mdecode(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m embeddings[documents\u001b[39m.\u001b[39;49mlabels_ \u001b[39m==\u001b[39;49m i]\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49margsort()[\u001b[39m-\u001b[39;49m\u001b[39m10\u001b[39;49m:][::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]])\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(topic_words)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m topic_words \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(best_k):\n\u001b[0;32m----> 8\u001b[0m     topic_words\u001b[39m.\u001b[39mappend([embed_model\u001b[39m.\u001b[39;49mdecode(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m embeddings[documents\u001b[39m.\u001b[39mlabels_ \u001b[39m==\u001b[39m i]\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39margsort()[\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:][::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(topic_words)\n",
      "File \u001b[0;32m/run/media/joramas/DATA/THESIS/playground/pipeline/embeddings/basic_embeddings.py:20\u001b[0m, in \u001b[0;36mEmbedding.decode\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(sentences, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     18\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecode(sentences)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "# select the topic words and decode them\n",
    "topic_words = []\n",
    "for i in range(best_k):\n",
    "    topic_words.append([embed_model.decode(word) for word in embeddings[documents.labels_ == i].mean(axis=0).argsort()[-10:][::-1]])\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k * vocab\n",
    "X_per_cluster = model.transform(documents)\n",
    "# D * vocab\n",
    "X_origin = self.vectorizer_model.transform(origin_documents)\n",
    "\n",
    "if self.word_select_method == 'tfidf_idfi':\n",
    "    socres = TFIDF_IDFi(X_per_cluster, X_origin, documents).socre()\n",
    "elif self.word_select_method == 'tfidf_tfi':\n",
    "    socres = TFIDF_TFi(X_per_cluster, X_origin, all_documents).socre()\n",
    "elif self.word_select_method == 'tfi':\n",
    "    socres = TFi(X_per_cluster).socre()\n",
    "elif self.word_select_method == 'tfidfi':\n",
    "    socres = TFIDFi(X_per_cluster).socre()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

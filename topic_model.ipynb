{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_repo_id=\"/mnt/DATA/THESIS/playground/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    model_file=\"./models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    model_type=\"mistral\", \n",
    "    gpu_layers=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence.split(\"|\")[-1]\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        sentences[i] = sentence.split(\"|\")[-1]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load the dataset from a folder\n",
    "dataset_folder = \"documents/topic_model_dataset\"\n",
    "sentences = []\n",
    "\n",
    "# Iterate over files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(dataset_folder, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            sentences.extend(preprocess_text(text))\n",
    "\n",
    "\n",
    "\n",
    "print(f'length of documents: {len(sentences)}')\n",
    "print(f\"first sentences:\\n\", \"\\n\".join(sentences[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    embeddings.append(embed_model.encode({\"text\":sentence, \"source\":\"test\"}))\n",
    "\n",
    "print(embeddings[:5])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "\n",
    "# embeddings = umap.UMAP(n_neighbors=15, \n",
    "#                     n_components=10, \n",
    "#                     min_dist=0.0, \n",
    "#                     metric='cosine',\n",
    "#                    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "scores = []\n",
    "for k in range(5,6):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)\n",
    "    score = silhouette_score(embeddings, kmeans.labels_)\n",
    "    scores.append(score)\n",
    "\n",
    "best_k = scores.index(max(scores)) + 5 \n",
    "documents = KMeans(n_clusters=best_k, random_state=42).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.embeddings.basic_embeddings import Embedding\n",
    "\n",
    "embed_model = Embedding()\n",
    "\n",
    "# select the topic words and decode them\n",
    "topic_words = []\n",
    "for i in range(best_k):\n",
    "    topic_words.append([embed_model.decode(word) for word in embeddings[documents.labels_ == i].mean(axis=0).argsort()[-10:][::-1]])\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k * vocab\n",
    "X_per_cluster = model.transform(documents)\n",
    "# D * vocab\n",
    "X_origin = self.vectorizer_model.transform(origin_documents)\n",
    "\n",
    "if self.word_select_method == 'tfidf_idfi':\n",
    "    socres = TFIDF_IDFi(X_per_cluster, X_origin, documents).socre()\n",
    "elif self.word_select_method == 'tfidf_tfi':\n",
    "    socres = TFIDF_TFi(X_per_cluster, X_origin, all_documents).socre()\n",
    "elif self.word_select_method == 'tfi':\n",
    "    socres = TFi(X_per_cluster).socre()\n",
    "elif self.word_select_method == 'tfidfi':\n",
    "    socres = TFIDFi(X_per_cluster).socre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline.pseudo_oracle.oracle.SentenceClassiier as p\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 3 4 5 6 7 8 9 10 13 14 15 35 37 39 41 42 47 50 53 62 82 83 118 138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
